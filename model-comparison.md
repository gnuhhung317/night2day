# So SÃ¡nh Chi Tiáº¿t Hai MÃ´ HÃ¬nh CycleGAN Night-to-Day

## Tá»•ng Quan

Repository nÃ y chá»©a hai phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ chuyá»ƒn Ä‘á»•i áº£nh Ä‘Ãªm sang áº£nh ban ngÃ y sá»­ dá»¥ng kiáº¿n trÃºc CycleGAN:

1. **CycleGAN vá»›i ResNet Generator** - Kiáº¿n trÃºc chuáº©n vá»›i ResNet blocks
2. **Multi-Scale CycleGAN** - Kiáº¿n trÃºc custom vá»›i multi-scale discriminator

---

## 1. CycleGAN with ResNet Generator

### ğŸ—ï¸ Kiáº¿n TrÃºc MÃ´ HÃ¬nh

#### Generator (ResNet-based U-Net)

**ThÃ´ng sá»‘ cÆ¡ báº£n:**
- Input shape: `256Ã—256Ã—3`
- Base filters: `64`
- Kiáº¿n trÃºc: Encoder-Decoder vá»›i Residual blocks

**Cáº¥u trÃºc chi tiáº¿t:**

```
Input (256Ã—256Ã—3)
    â†“
Reflection Padding + Conv2D(64, 7Ã—7) + GroupNorm + ReLU
    â†“
Downsampling (2 blocks)
â”œâ”€ Conv2D(128, 3Ã—3, stride=2) + GroupNorm + ReLU
â””â”€ Conv2D(256, 3Ã—3, stride=2) + GroupNorm + ReLU
    â†“
Residual Blocks (9 blocks)
â”œâ”€ [ReflectionPad â†’ Conv2D â†’ GroupNorm â†’ ReLU]
â”œâ”€ [ReflectionPad â†’ Conv2D â†’ GroupNorm]
â””â”€ Add with input (skip connection)
    â†“
Upsampling (2 blocks)
â”œâ”€ Conv2DTranspose(128, 3Ã—3, stride=2) + GroupNorm + ReLU
â””â”€ Conv2DTranspose(64, 3Ã—3, stride=2) + GroupNorm + ReLU
    â†“
Reflection Padding + Conv2D(3, 7Ã—7)
    â†“
Tanh activation
    â†“
Output (256Ã—256Ã—3) range: [-1, 1]
```

**Äáº·c Ä‘iá»ƒm ná»•i báº­t:**
- âœ… **Reflection Padding**: TrÃ¡nh artifacts á»Ÿ biÃªn áº£nh
- âœ… **Residual Blocks**: GiÃºp há»c identity mapping, training á»•n Ä‘á»‹nh hÆ¡n
- âœ… **GroupNormalization** (groups=-1): TÆ°Æ¡ng Ä‘Æ°Æ¡ng Instance Normalization
- âœ… **Skip connections**: Giá»¯ thÃ´ng tin chi tiáº¿t tá»« encoder

#### Discriminator (PatchGAN)

**ThÃ´ng sá»‘:**
- Base filters: `64`
- Kernel size: `4Ã—4`
- Downsampling blocks: `3`

**Cáº¥u trÃºc:**

```
Input (256Ã—256Ã—3)
    â†“
Conv2D(64, 4Ã—4, stride=2) + LeakyReLU(0.2)
    â†“
Conv2D(128, 4Ã—4, stride=2) + GroupNorm + LeakyReLU(0.2)
    â†“
Conv2D(256, 4Ã—4, stride=2) + GroupNorm + LeakyReLU(0.2)
    â†“
Conv2D(512, 4Ã—4, stride=1) + GroupNorm + LeakyReLU(0.2)
    â†“
Conv2D(1, 4Ã—4, stride=1)
    â†“
Output: Patch-based classification
```

**Äáº·c Ä‘iá»ƒm:**
- ğŸ¯ **PatchGAN**: ÄÃ¡nh giÃ¡ tÃ­nh real/fake theo tá»«ng patch, khÃ´ng pháº£i toÃ n bá»™ áº£nh
- ğŸ¯ **LeakyReLU**: TrÃ¡nh dead neurons

### ğŸ“ Training Configuration

#### Loss Functions

```python
# 1. Generator Adversarial Loss
L_adv = MSE(ones, D(G(x)))

# 2. Cycle Consistency Loss (Î»_cycle = 10.0)
L_cycle = MAE(x, F(G(x))) + MAE(y, G(F(y)))
L_cycle_weighted = L_cycle Ã— 10.0

# 3. Identity Loss (Î»_identity = 0.5)
L_identity = MAE(y, G(y)) + MAE(x, F(x))
L_identity_weighted = L_identity Ã— 10.0 Ã— 0.5

# 4. Total Generator Loss
L_G_total = L_adv + L_cycle_weighted + L_identity_weighted

# 5. Discriminator Loss
L_D = MSE(ones, D(real)) + MSE(zeros, D(fake))
```

#### Hyperparameters

| Parameter | Value |
|-----------|-------|
| **Batch Size** | 1 |
| **Image Size** | 256Ã—256 |
| **Learning Rate** | 2e-4 |
| **Optimizer** | Adam (Î²â‚=0.5, Î²â‚‚=0.999) |
| **Epochs** | 100 |
| **Î»_cycle** | 10.0 |
| **Î»_identity** | 0.5 |
| **Dataset** | BDD100K |
| **Train samples** | 1200 per domain |
| **Test samples** | 400 per domain |

#### Data Preprocessing

```python
# Training augmentation
- Random horizontal flip
- Resize to 720Ã—720
- Random crop to 256Ã—256
- Normalize: (img/127.5) - 1

# Test preprocessing
- Resize to 256Ã—256
- Normalize: (img/127.5) - 1
```

---

## 2. Multi-Scale CycleGAN

### ğŸ—ï¸ Kiáº¿n TrÃºc MÃ´ HÃ¬nh

#### Generator (Custom U-Net with Inception Modules)

**ThÃ´ng sá»‘ cÆ¡ báº£n:**
- Input shape: `256Ã—256Ã—3`
- Base filters: `16`
- Kernel size: `5Ã—5` (toÃ n bá»™)
- Kiáº¿n trÃºc: U-Net vá»›i Inception modules

**Inception Module:**

```python
def inceptionModule(inputs, filter):
    x1 = Conv2D(filter, 5Ã—5, dilation=1) â†’ Activation â†’ GroupNorm
    x2 = Conv2D(filter, 5Ã—5, dilation=1) â†’ Activation â†’ GroupNorm
    x3 = Conv2D(filter, 5Ã—5, dilation=1) â†’ Activation â†’ GroupNorm
    return x3
```

**Cáº¥u trÃºc Generator:**

```
Input (256Ã—256Ã—3)
    â†“
Encoder Block 1: InceptionModule(16) â†’ MaxPool â†’ (128Ã—128Ã—16)
    â†“ skip1
Encoder Block 2: InceptionModule(32) â†’ MaxPool â†’ (64Ã—64Ã—32)
    â†“ skip2
Encoder Block 3: InceptionModule(64) â†’ MaxPool â†’ (32Ã—32Ã—64)
    â†“ skip3
Encoder Block 4: InceptionModule(128) â†’ MaxPool â†’ (16Ã—16Ã—128)
    â†“ skip4
Encoder Block 5: InceptionModule(256) â†’ MaxPool â†’ (8Ã—8Ã—256)
    â†“ skip5
Latent Space:
â”œâ”€ Flatten
â”œâ”€ Dense(128, L2=0.001) â† Bottleneck
â”œâ”€ Dense(8Ã—8Ã—256, L2=0.001)
â””â”€ Reshape(8Ã—8Ã—256)
    â†“
Decoder Block 1: Upsample + skip5 + InceptionModule(256) â†’ (16Ã—16Ã—256)
    â†“
Decoder Block 2: Upsample + skip4 + InceptionModule(128) â†’ (32Ã—32Ã—128)
    â†“
Decoder Block 3: Upsample + skip3 + InceptionModule(64) â†’ (64Ã—64Ã—64)
    â†“
Decoder Block 4: Upsample + skip2 + InceptionModule(32) â†’ (128Ã—128Ã—32)
    â†“
Decoder Block 5: Upsample + skip1 + InceptionModule(16) â†’ (256Ã—256Ã—16)
    â†“
Multi-scale Fusion:
â”œâ”€ Conv2DTranspose(16, stride=16) from Decoder Block 1
â””â”€ Concatenate with Decoder Block 5
    â†“
Conv2DTranspose(3, 5Ã—5, stride=1)
    â†“
Sigmoid activation
    â†“
Output (256Ã—256Ã—3) range: [0, 1]
```

**Äáº·c Ä‘iá»ƒm ná»•i báº­t:**
- âœ… **Inception Modules**: Multiple convolutions há»c features phong phÃº hÆ¡n
- âœ… **Latent Bottleneck**: Dense layer 128 units vá»›i L2 regularization
- âœ… **Multi-scale Fusion**: Káº¿t há»£p features tá»« decoder sÃ¢u (coarse) vá»›i decoder nÃ´ng (fine)
- âœ… **Large Kernel (5Ã—5)**: Há»c quan há»‡ pixel vá»›i neighbors xa hÆ¡n

#### Discriminator (Multi-Scale PatchGAN) â­

**Äáº·c Ä‘iá»ƒm Ä‘á»™c Ä‘Ã¡o:**
- ğŸŒŸ **3 outputs á»Ÿ cÃ¡c scales khÃ¡c nhau**
- ğŸŒŸ Kernel size: `5Ã—5` (lá»›n hÆ¡n chuáº©n)

**Cáº¥u trÃºc:**

```
Input (256Ã—256Ã—3)
    â†“
Encoder Block 1: InceptionModule(16) â†’ MaxPool â†’ (128Ã—128Ã—16)
    â†“
Encoder Block 2: InceptionModule(32) â†’ MaxPool â†’ (64Ã—64Ã—32)
    â†“
Encoder Block 3: InceptionModule(64) â†’ MaxPool â†’ (32Ã—32Ã—64) â”€â”
    â†“                                                          â”‚
Encoder Block 4: InceptionModule(128) â†’ MaxPool â†’ (16Ã—16Ã—128) â”¤
    â†“                                                          â”‚
Encoder Block 5: InceptionModule(256) â†’ MaxPool â†’ (8Ã—8Ã—256)   â”‚
    â†“                                                          â†“
Output 1: Conv2D(1, 5Ã—5) â†’ (8Ã—8Ã—1)   â† Coarse scale (objects xa)
    â†“                                                          â†“
Output 2: Conv2D(1, 5Ã—5) â†’ (16Ã—16Ã—1) â† Medium scale
    â†“                                                          â†“
Output 3: Conv2D(1, 5Ã—5) â†’ (32Ã—32Ã—1) â† Fine scale (objects gáº§n)
```

**Ã nghÄ©a Multi-Scale:**
- ğŸ“ **Scale 1 (8Ã—8)**: ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ cáº£nh, objects xa camera
- ğŸ“ **Scale 2 (16Ã—16)**: ÄÃ¡nh giÃ¡ objects á»Ÿ khoáº£ng cÃ¡ch trung bÃ¬nh
- ğŸ“ **Scale 3 (32Ã—32)**: ÄÃ¡nh giÃ¡ chi tiáº¿t, objects gáº§n camera

**Lá»£i Ã­ch:**
> "Discriminator pháº£i nháº­n biáº¿t toÃ n bá»™ object, dÃ¹ gáº§n hay xa camera. Trong áº£nh lÃ¡i xe, objects cÃ³ thá»ƒ á»Ÿ nhiá»u khoáº£ng cÃ¡ch khÃ¡c nhau. Multi-scale giÃºp Ä‘Ã¡nh giÃ¡ Ä‘Ãºng á»Ÿ má»i scale."

### ğŸ“ Training Configuration

#### Loss Functions

```python
# Discriminator cÃ³ 3 outputs â†’ 3 losses
L_D_source = MSE_scale1 + MSE_scale2 + MSE_scale3
L_D_target = MSE_scale1 + MSE_scale2 + MSE_scale3

# Generator adversarial loss (3 scales)
L_adv_target = MSE_scale1 + MSE_scale2 + MSE_scale3
L_adv_source = MSE_scale1 + MSE_scale2 + MSE_scale3

# Cycle consistency
L_cycle = MAE(x, F(G(x))) Ã— 10 + MAE(y, G(F(y))) Ã— 10

# Identity
L_identity = MAE(y, G(y)) Ã— 0.5 + MAE(x, F(x)) Ã— 0.5

# Total GAN loss
L_GAN_total = L_adv Ã— 1 + L_cycle Ã— 10 + L_identity Ã— 0.5
```

**Loss weights:**
```python
loss_weights = [
    1, 1, 1,      # d_target_re scales 1,2,3
    1, 1, 1,      # d_source_re scales 1,2,3
    10, 10,       # cycle consistency (forward, backward)
    0.5, 0.5      # identity (source, target)
]
```

#### Hyperparameters

| Parameter | Value |
|-----------|-------|
| **Batch Size** | 4 |
| **Image Size** | 256Ã—256 |
| **Learning Rate (Discriminator)** | 1e-4 |
| **Learning Rate (Generator)** | 5e-5 |
| **Optimizer** | Adam |
| **Weight Decay** | 6e-8 |
| **Epochs** | 30,000 ğŸ”¥ |
| **Base Filters** | 16 |
| **Kernel Size** | 5Ã—5 |
| **Dataset** | BDD100K |
| **Checkpoint Interval** | Every 2000 epochs |
| **Image Logging** | Every 1000 epochs |

#### Data Preprocessing

```python
# Simple preprocessing
- Resize to 256Ã—256
- Normalize: img/255 (range [0,1])
```

#### Training vá»›i WandB ğŸ“Š

```python
wandb.init(
    project="night2day-cyclegan",
    config={
        "epochs": 30000,
        "batch_size": 4,
        "learning_rate": 0.0001,
        "architecture": "Multi-Scale CycleGAN"
    }
)

# Tracked metrics
- Discriminator losses (source, target, 3 scales each)
- Generator losses (source, target, 3 scales each)
- Cycle consistency losses
- Identity losses
- Generated images every 1000 epochs
- Auto-backup checkpoints má»—i 2000 epochs
```

**Features WandB:**
- âœ… Real-time loss tracking
- âœ… Auto-save checkpoints lÃªn cloud
- âœ… Visualize generated images
- âœ… Resume training tá»± Ä‘á»™ng khi Kaggle timeout
- âœ… Compare multiple runs

---

## ğŸ“Š So SÃ¡nh Chi Tiáº¿t

### Kiáº¿n TrÃºc

| Aspect | ResNet CycleGAN | Multi-Scale CycleGAN |
|--------|-----------------|----------------------|
| **Generator Base** | ResNet blocks | Inception modules |
| **Generator Filters** | 64 â†’ 256 | 16 â†’ 256 |
| **Generator Kernel** | 3Ã—3, 4Ã—4, 7Ã—7 | 5Ã—5 (uniform) |
| **Latent Space** | âŒ None | âœ… Dense(128) + L2 reg |
| **Multi-scale Fusion** | âŒ None | âœ… Skip tá»« deep decoder |
| **Output Activation** | tanh [-1,1] | sigmoid [0,1] |
| **Discriminator Type** | Single-scale PatchGAN | Multi-scale PatchGAN |
| **Discriminator Outputs** | 1 | 3 (scales: 8Ã—8, 16Ã—16, 32Ã—32) |
| **Discriminator Kernel** | 4Ã—4 | 5Ã—5 |

### Training

| Aspect | ResNet CycleGAN | Multi-Scale CycleGAN |
|--------|-----------------|----------------------|
| **Batch Size** | 1 | 4 |
| **Learning Rate** | 2e-4 | 1e-4 (D), 5e-5 (G) |
| **Epochs** | 100 | 30,000 |
| **Training Time** | ~Few hours | ~Days/Weeks |
| **Data Augmentation** | Heavy (flip, crop) | Light (resize only) |
| **Normalization** | [-1, 1] | [0, 1] |
| **Optimizer** | Adam (Î²â‚=0.5) | Adam (default Î²) |
| **Monitoring** | Manual callbacks | WandB cloud tracking |
| **Checkpointing** | Local only | Local + Cloud |

### Loss Functions

| Loss Component | ResNet CycleGAN | Multi-Scale CycleGAN |
|----------------|-----------------|----------------------|
| **Adversarial** | MSE | MSE (Ã—3 scales) |
| **Cycle Consistency** | MAE Ã— 10 | MAE Ã— 10 |
| **Identity** | MAE Ã— 5 | MAE Ã— 0.5 |
| **Regularization** | âŒ None | âœ… L2 (0.001) in latent |
| **Total Losses** | 4 | 10 (3 scales Ã— 2 + cycle + identity) |

---

## ğŸ¯ Æ¯u NhÆ°á»£c Äiá»ƒm

### ResNet CycleGAN

**Æ¯u Ä‘iá»ƒm:**
- âœ… **Proven architecture**: Kiáº¿n trÃºc Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£
- âœ… **Training nhanh**: Chá»‰ 100 epochs
- âœ… **á»”n Ä‘á»‹nh**: Residual blocks giÃºp gradient flow tá»‘t
- âœ… **Reflection padding**: KhÃ´ng artifacts á»Ÿ biÃªn
- âœ… **Dá»… implement**: Code Ä‘Æ¡n giáº£n, dá»… hiá»ƒu
- âœ… **Resource-friendly**: Batch size 1, Ã­t RAM

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ **Single-scale discriminator**: KhÃ´ng xá»­ lÃ½ tá»‘t objects á»Ÿ nhiá»u khoáº£ng cÃ¡ch
- âŒ **Ãt data augmentation**: KhÃ´ng diverse data
- âŒ **No latent regularization**: KhÃ´ng force compact representation
- âŒ **Short training**: 100 epochs cÃ³ thá»ƒ chÆ°a Ä‘á»§

### Multi-Scale CycleGAN

**Æ¯u Ä‘iá»ƒm:**
- âœ… **Multi-scale discriminator**: Xá»­ lÃ½ objects gáº§n/xa camera cá»±c tá»‘t
- âœ… **Large kernel (5Ã—5)**: Há»c quan há»‡ pixel-neighbor tá»‘t hÆ¡n
- âœ… **Latent bottleneck**: Force compact, meaningful representation
- âœ… **Multi-scale fusion**: Káº¿t há»£p coarse + fine features
- âœ… **WandB integration**: Track metrics, auto-resume, cloud backup
- âœ… **Long training**: 30k epochs â†’ better convergence
- âœ… **Batch size 4**: Stable gradient estimates

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ **Training ráº¥t lÃ¢u**: 30,000 epochs
- âŒ **Phá»©c táº¡p**: KhÃ³ debug, nhiá»u hyperparameters
- âŒ **Resource-intensive**: Cáº§n GPU máº¡nh, thá»i gian lÃ¢u
- âŒ **Overfitting risk**: Training quÃ¡ lÃ¢u cÃ³ thá»ƒ overfit
- âŒ **3 discriminator outputs**: TÃ­nh toÃ¡n loss phá»©c táº¡p hÆ¡n

---

## ğŸ”¬ Thiáº¿t Káº¿ Äáº·c Biá»‡t cho BÃ i ToÃ¡n Night-to-Day

### Multi-Scale Discriminator Philosophy

**Váº¥n Ä‘á»:**
> Trong áº£nh lÃ¡i xe ban Ä‘Ãªm, má»™t sá»‘ váº­t thá»ƒ cÃ³ thá»ƒ ráº¥t tá»‘i do thiáº¿u Ã¡nh sÃ¡ng. Chá»‰ má»™t pháº§n nhá» cá»§a váº­t thá»ƒ visible. Discriminator pháº£i há»c Ä‘Æ°á»£c cÃ¡ch Ä‘Ã¡nh giÃ¡ liá»‡u viá»‡c reconstruct váº­t thá»ƒ ban ngÃ y cÃ³ Ä‘Ãºng hay khÃ´ng.

**Giáº£i phÃ¡p:**

1. **Kernel 5Ã—5**: 
   - Má»—i pixel Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ vá»›i context cá»§a 24 neighbors (5Ã—5-1)
   - Há»c Ä‘Æ°á»£c "pixel nÃ y vá»›i neighbors cá»§a nÃ³ cÃ³ há»£p lÃ½ khÃ´ng?"
   - Tá»‘t hÆ¡n kernel 3Ã—3 cho viá»‡c reconstruct tá»« partial info

2. **3 Scales**:
   - **8Ã—8 (coarse)**: Objects xa camera (xe phÃ­a xa, biá»ƒn bÃ¡o xa)
   - **16Ã—16 (medium)**: Objects khoáº£ng cÃ¡ch trung bÃ¬nh
   - **32Ã—32 (fine)**: Objects gáº§n camera (Ä‘Æ°á»ng phÃ­a trÆ°á»›c, xe ngay cáº¡nh)

3. **Inception Modules**:
   - 3 convolutions liÃªn tiáº¿p â†’ há»c features á»Ÿ multiple levels
   - Káº¿t há»£p vá»›i 1Ã—1 conv (residual) â†’ skip useless transformations

### Latent Space Regularization

```python
x = Flatten(conv5)
x = Dense(128, L2=0.001)  â† Force compact representation
x = Dense(original_size, L2=0.001)
x = Reshape(...)
```

**Má»¥c Ä‘Ã­ch:**
- Buá»™c model há»c compact, meaningful representation
- KhÃ´ng cho phÃ©p memorize patterns
- Generalization tá»‘t hÆ¡n

---

## ğŸ“ˆ Káº¿t Quáº£ Dá»± Kiáº¿n

### ResNet CycleGAN
- âš¡ **Training time**: 2-4 giá» (vá»›i GPU)
- ğŸ¨ **Quality**: Good, Ä‘á»§ cho most cases
- ğŸ“Š **Use case**: Prototyping, baseline, limited resources

### Multi-Scale CycleGAN
- ğŸ• **Training time**: VÃ i ngÃ y Ä‘áº¿n vÃ i tuáº§n
- ğŸ¨ **Quality**: Excellent, chi tiáº¿t objects gáº§n/xa
- ğŸ“Š **Use case**: Production, research, khi cáº§n quality cao nháº¥t

---

## ğŸ’¡ Recommendations

### Khi nÃ o dÃ¹ng ResNet CycleGAN?
- âœ… Cáº§n káº¿t quáº£ nhanh
- âœ… Limited GPU resources
- âœ… Baseline model
- âœ… áº¢nh khÃ´ng cÃ³ quÃ¡ nhiá»u objects á»Ÿ different distances

### Khi nÃ o dÃ¹ng Multi-Scale CycleGAN?
- âœ… Cáº§n quality cao nháº¥t
- âœ… CÃ³ GPU máº¡nh vÃ  thá»i gian
- âœ… áº¢nh cÃ³ nhiá»u objects á»Ÿ different scales (driving scenes)
- âœ… Production deployment
- âœ… Research purposes

---

## ğŸ› ï¸ Technical Insights

### GroupNormalization vs BatchNormalization

**Táº¡i sao dÃ¹ng GroupNorm?**
- Batch size = 1 â†’ BatchNorm khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t
- GroupNorm (groups=-1) = Instance Normalization
- Normalize tá»«ng channel Ä‘á»™c láº­p
- KhÃ´ng phá»¥ thuá»™c batch size

### Reflection Padding vs Zero Padding

**ResNet model dÃ¹ng Reflection Padding:**
```
Original: [1, 2, 3, 4, 5]
Zero Pad: [0, 0, 1, 2, 3, 4, 5, 0, 0]
Reflect:  [3, 2, 1, 2, 3, 4, 5, 4, 3]
```
- âœ… KhÃ´ng táº¡o artifacts á»Ÿ biÃªn
- âœ… Smooth transitions

### PatchGAN vs PixelGAN

**PatchGAN advantages:**
- ÄÃ¡nh giÃ¡ NÃ—N patches thay vÃ¬ whole image
- Fewer parameters
- Better for high-frequency details
- Computational efficient

---

## ğŸ“š Dataset: BDD100K

**Berkeley DeepDrive Dataset:**
- 100K diverse driving images
- Day/Night/Dawn/Dusk conditions
- Various weather conditions
- Urban/Highway/Residential areas

**Split trong cÃ¡c models:**
- **ResNet**: 1200 train, 400 test (má»—i domain)
- **Multi-Scale**: Full dataset, random sampling má»—i batch

---

## ğŸ”® Future Improvements

### CÃ³ thá»ƒ thá»­:
1. **Attention Mechanisms**: Self-attention cho generator
2. **Progressive Growing**: Train tá»« low â†’ high resolution
3. **StyleGAN features**: Style mixing, AdaIN
4. **Perceptual Loss**: VGG-based thay vÃ¬ MAE
5. **Color Histogram Matching**: Post-processing
6. **Temporal Consistency**: Náº¿u cÃ³ video sequences

---

## ğŸ“„ TÃ i Liá»‡u Tham Kháº£o

- **CycleGAN Paper**: "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks" (Zhu et al., 2017)
- **ResNet Paper**: "Deep Residual Learning for Image Recognition" (He et al., 2016)
- **PatchGAN**: "Image-to-Image Translation with Conditional Adversarial Networks" (Isola et al., 2017)
- **BDD100K**: https://bdd-data.berkeley.edu/

---

## ğŸ“ Credits

- **Framework**: TensorFlow 2.x / Keras 3.x
- **Dataset**: BDD100K (Berkeley DeepDrive)
- **Experiment Tracking**: Weights & Biases
- **Platform**: Kaggle

---

*Táº¡o ngÃ y: January 3, 2026*
