{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4760aad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:30:25.020257Z",
     "iopub.status.busy": "2025-12-08T08:30:25.019895Z",
     "iopub.status.idle": "2025-12-08T08:30:25.024831Z",
     "shell.execute_reply": "2025-12-08T08:30:25.024146Z"
    },
    "papermill": {
     "duration": 0.014496,
     "end_time": "2025-12-08T08:30:25.026677",
     "exception": false,
     "start_time": "2025-12-08T08:30:25.012181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://keras.io/examples/generative/cyclegan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2992996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:28:54.441537Z",
     "iopub.status.busy": "2025-12-08T08:28:54.441231Z",
     "iopub.status.idle": "2025-12-08T08:29:05.446120Z",
     "shell.execute_reply": "2025-12-08T08:29:05.445192Z",
     "shell.execute_reply.started": "2025-12-08T08:28:54.441509Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-12-08T08:30:25.032640",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import tf_keras as keras\n",
    "import keras\n",
    "import os\n",
    "#os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0068ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tf.keras.layers.GroupNormalization(groups=-1)\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/GroupNormalization\n",
    "#Relation to Instance Normalization: If the number of groups is set to the input dimension (number of groups is equal to number of channels), then this operation becomes identical to Instance Normalization. You can achieve this via groups=-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be2a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:29:15.162413Z",
     "iopub.status.busy": "2025-12-08T08:29:15.161452Z",
     "iopub.status.idle": "2025-12-08T08:29:15.400444Z",
     "shell.execute_reply": "2025-12-08T08:29:15.399422Z",
     "shell.execute_reply.started": "2025-12-08T08:29:15.162379Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd91e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:29:19.133014Z",
     "iopub.status.busy": "2025-12-08T08:29:19.132315Z",
     "iopub.status.idle": "2025-12-08T08:29:19.137783Z",
     "shell.execute_reply": "2025-12-08T08:29:19.136803Z",
     "shell.execute_reply.started": "2025-12-08T08:29:19.132979Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/input/solesensei_bdd100k/bdd100k/bdd100k/images/100k'\n",
    "\n",
    "trainA_path = os.path.join(dataset_path, 'train/trainA')\n",
    "trainB_path = os.path.join(dataset_path, 'train/trainB')\n",
    "\n",
    "testA_path = os.path.join(dataset_path, 'train/testA')\n",
    "testB_path = os.path.join(dataset_path, 'train/testB')\n",
    "\n",
    "BUFFER_SIZE = 256\n",
    "batch_size = 1\n",
    "img_size = (256, 256)\n",
    "orig_img_size = (720, 720)\n",
    "input_img_size = (256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4376a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#if os.path.exists('/kaggle/working/Model'):\n",
    "#    shutil.rmtree('/kaggle/working/Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f030a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:29:27.333058Z",
     "iopub.status.busy": "2025-12-08T08:29:27.332253Z",
     "iopub.status.idle": "2025-12-08T08:29:27.337727Z",
     "shell.execute_reply": "2025-12-08T08:29:27.336813Z",
     "shell.execute_reply.started": "2025-12-08T08:29:27.333023Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_img_dir = '/kaggle/working/Model/Model_Data/save'\n",
    "output_ckpt_dir = '/kaggle/working/Model/Model_Data/ckpt'\n",
    "backup_dir = '/kaggle/working/Model/Model_Data/backup'\n",
    "\n",
    "for i in (output_img_dir, output_ckpt_dir, backup_dir):\n",
    "    if not os.path.exists(i):\n",
    "        os.makedirs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d290fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A -> Day\n",
    "# B -> Night"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41590b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:39:17.785785Z",
     "iopub.status.busy": "2024-10-22T17:39:17.785299Z",
     "iopub.status.idle": "2024-10-22T17:39:18.04488Z",
     "shell.execute_reply": "2024-10-22T17:39:18.043603Z",
     "shell.execute_reply.started": "2024-10-22T17:39:17.785742Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "train_day = glob.glob(os.path.join(dataset_path, 'train/trainA/*.jpg'))\n",
    "train_night = glob.glob(os.path.join(dataset_path, 'train/trainB/*.jpg'))\n",
    "\n",
    "print(len(train_day), len(train_night))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e47ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:39:18.047913Z",
     "iopub.status.busy": "2024-10-22T17:39:18.046813Z",
     "iopub.status.idle": "2024-10-22T17:39:18.086223Z",
     "shell.execute_reply": "2024-10-22T17:39:18.08493Z",
     "shell.execute_reply.started": "2024-10-22T17:39:18.047851Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "val_day = glob.glob(os.path.join(dataset_path, 'train/testA/*.jpg'))\n",
    "val_night = glob.glob(os.path.join(dataset_path, 'train/testB/*.jpg'))\n",
    "\n",
    "print(len(val_day), len(val_night))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a8b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:39:18.088176Z",
     "iopub.status.busy": "2024-10-22T17:39:18.08777Z",
     "iopub.status.idle": "2024-10-22T17:39:18.096422Z",
     "shell.execute_reply": "2024-10-22T17:39:18.095162Z",
     "shell.execute_reply.started": "2024-10-22T17:39:18.088135Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "train_day = train_day[::17]\n",
    "train_night = train_night[::13]\n",
    "print(len(train_day),len(train_night))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1dc2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:39:18.098278Z",
     "iopub.status.busy": "2024-10-22T17:39:18.097846Z",
     "iopub.status.idle": "2024-10-22T17:39:18.110582Z",
     "shell.execute_reply": "2024-10-22T17:39:18.10928Z",
     "shell.execute_reply.started": "2024-10-22T17:39:18.098236Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "val_day = val_day[::10]\n",
    "val_night = val_night[::6]\n",
    "print(len(val_day),len(val_night))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551a16d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46df79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:29:42.864338Z",
     "iopub.status.busy": "2025-12-08T08:29:42.863541Z",
     "iopub.status.idle": "2025-12-08T08:29:56.523376Z",
     "shell.execute_reply": "2025-12-08T08:29:56.522670Z",
     "shell.execute_reply.started": "2025-12-08T08:29:42.864303Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainB = tf.data.Dataset.list_files(os.path.join(dataset_path, 'test/trainA/*jpg'), shuffle=True).take(1200)\n",
    "trainA = tf.data.Dataset.list_files(os.path.join(dataset_path, 'test/trainB/*jpg'), shuffle=True).take(1200)\n",
    "\n",
    "testB = tf.data.Dataset.list_files(os.path.join(dataset_path, 'test/testA/*jpg'), shuffle=True).take(400)\n",
    "testA = tf.data.Dataset.list_files(os.path.join(dataset_path, 'test/testB/*jpg'), shuffle=True).take(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67092ce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:29:56.530268Z",
     "iopub.status.busy": "2025-12-08T08:29:56.530045Z",
     "iopub.status.idle": "2025-12-08T08:29:56.538933Z",
     "shell.execute_reply": "2025-12-08T08:29:56.538031Z",
     "shell.execute_reply.started": "2025-12-08T08:29:56.530246Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951f146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T08:29:56.551575Z",
     "iopub.status.busy": "2025-12-08T08:29:56.551297Z",
     "iopub.status.idle": "2025-12-08T08:29:56.559849Z",
     "shell.execute_reply": "2025-12-08T08:29:56.558973Z",
     "shell.execute_reply.started": "2025-12-08T08:29:56.551536Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "\n",
    "def preprocess_train_image(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.resize(img, [*orig_img_size])\n",
    "    img = tf.image.random_crop(img, size=[*img_size, 3])\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_test_image(img):\n",
    "    img = tf.image.resize(img, [img_size[0], img_size[1]])\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "def map_train_load(image):\n",
    "    image = load(image)\n",
    "    image = preprocess_train_image(image)\n",
    "    return image\n",
    "\n",
    "def map_test_load(image):\n",
    "    image = load(image)\n",
    "    image = preprocess_test_image(image)\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea998de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainA = (\n",
    "    trainA.map(map_train_load, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "trainB = (\n",
    "    trainB.map(map_train_load, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "testA = (\n",
    "    testA.map(map_test_load, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "testB = (\n",
    "    testB.map(map_test_load, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce31a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:39:18.11251Z",
     "iopub.status.busy": "2024-10-22T17:39:18.112118Z",
     "iopub.status.idle": "2024-10-22T17:39:18.122418Z",
     "shell.execute_reply": "2024-10-22T17:39:18.121171Z",
     "shell.execute_reply.started": "2024-10-22T17:39:18.112468Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "def read_jpg(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "\n",
    "def normalize(input_image):\n",
    "    input_image = tf.cast(input_image, tf.float32)/127.5 - 1\n",
    "    return input_image\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = read_jpg(image_path)\n",
    "    image = tf.image.resize(image, (256, 256))\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1a754",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "for i, samples in enumerate(zip(trainA.take(2), trainB.take(2))):\n",
    "    day = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
    "    night = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
    "    ax[i, 0].imshow(day)\n",
    "    ax[i, 1].imshow(night)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e89b6e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2684b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReflectionPadding2D(layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fb0bb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1, gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1, gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1, gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1, gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750d9c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = tf.keras.layers.GroupNormalization(groups=-1, gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(img_input, x, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada96f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        (4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(1, 1),\n",
    "            )\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1244241",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_G = get_resnet_generator(name=\"generator_G\")\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bdfde",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "disc_X = get_discriminator(name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(name=\"discriminator_Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b3ac9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02fa98",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CycleGan(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is Day and y is Night\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real images through the generators and get the generated images\n",
    "        # 2. Pass the generated images back to the generators to check if we\n",
    "        #    can predict the original image from the generated image.\n",
    "        # 3. Do an identity mapping of the real images using the generators.\n",
    "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adversarial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Day to fake night\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            # Night to fake day -> y2x\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle (Day to fake night to fake day): x -> y -> x\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            # Cycle (Night to fake day to fake night) y -> x -> y\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adversarial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b521f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52eeec6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=2):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        for i, img in enumerate(testA.take(self.num_img)):\n",
    "            prediction = self.model.gen_G(img, training=False)[0].numpy()\n",
    "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "            ax[i, 0].imshow(img)\n",
    "            ax[i, 1].imshow(prediction)\n",
    "            ax[i, 0].set_title(\"Input image\")\n",
    "            ax[i, 1].set_title(\"Translated image\")\n",
    "            ax[i, 0].axis(\"off\")\n",
    "            ax[i, 1].axis(\"off\")\n",
    "\n",
    "            prediction = tf.keras.utils.array_to_img(prediction)\n",
    "            prediction.save(\n",
    "                f\"{output_img_dir}/generated_img_{i}_{epoch+1}.png\"\n",
    "            )\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcde0e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = output_ckpt_dir + \"/cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "backup_callback = keras.callbacks.BackupAndRestore(backup_dir=backup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99bcaf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6d560",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adv_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2f05f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a9c03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycle_gan_model.build((1, 256, 256, 3))\n",
    "\n",
    "cycle_gan_model.gen_G.build((1, 256, 256, 3))\n",
    "cycle_gan_model.gen_F.build((1, 256, 256, 3))\n",
    "cycle_gan_model.disc_X.build((1, 256, 256, 3))\n",
    "cycle_gan_model.disc_Y.build((1, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73498f03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((trainA, trainB)),\n",
    "    epochs=100,\n",
    "    callbacks=[plotter, model_checkpoint_callback, backup_callback]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 161598,
     "sourceId": 442057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-08T08:30:21.399997",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}